rm: cannot remove `dump.rdb': No such file or directory
Could not connect to Redis at 127.0.0.1:6363: Connection refused
+ job_id=7742758.master.cm.cluster
++ cut -d. -f1
+ job_number=7742758
+ echo 'Job id is 7742758'
+ echo 7742758
+ cd /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/logs
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/logs
+ REDIS_URL=redis://node34-024:6363
+ QMD_LOG_DIR=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs
+ OUTPUT_ERROR_DIR=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs
+ QMD_JOB=very_long_qhl_experimental_data_63
+ echo 'PBS job name is very_long_qhl_experimental_data_63'
+ QMD_LOG=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_63.qmd.7742758.log
+ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742758.master.cm.cluster
++ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742758.master.cm.cluster
+ export nodes=node34-024
+ nodes=node34-024
++ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742758.master.cm.cluster
++ wc -l
+ export nnodes=1
+ nnodes=1
+ export confile=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_63.conf
+ confile=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_63.conf
+ for i in '$nodes'
+ echo node34-024
+ cd /panfs/panasas01/phys/bf16951/QMD/Libraries/QML_lib
+ [[ node34-024 == \n\o\d\e* ]]
+ echo 'Launching RQ worker on remote nodes using mpirun'
+ printf 'In RUN script:  -agr non_interacting_ising'
+ mpirun -np 1 -machinefile /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_63.conf rq worker 63 -u redis://node34-024:6363
+ ((  0 == 1 ))
+ sleep 5
+ cd /panfs/panasas01/phys/bf16951/QMD/ExperimentalSimulations
++ date +%H:%M:%S
+ echo 'Starting Exp.py at 15:25:39; results dir: /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14'
+ echo 'CONFIG: -p=20000 -e=3000 -bt=1500 -rt=0.5 -ra=0.8 -cb=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/cumulative.csv -pt=1 -pgh=1.0 -qid=63 -rqt=10000 -pkl=0 -host=node34-024 -port=6363 -dir=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14 -log=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_63.qmd.7742758.log cpr=1 -ds=NVB_rescale_dataset.p -dst=5000 -dto=205 -ggr=two_qubit_ising_rotation_hyperfine_transverse'
+ python3 Exp.py -rq=1 -op=xTiPPyTiPPzTiPPxTxPPyTyPPzTz -p=20000 -e=3000 -bt=1500 -rt=0.5 -ra=0.8 -pgh=1.0 -qid=63 -rqt=200000 -g=1 -exp=1 -meas=hahn -qhl=1 -fq=0 -pt=1 -pkl=0 -host=node34-024 -port=6363 -dir=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14 -log=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_63.qmd.7742758.log -cb=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/cumulative.csv -cpr=1 -ds=NVB_rescale_dataset.p -dst=5000 -dto=205 -ggr=two_qubit_ising_rotation_hyperfine_transverse -agr non_interacting_ising -latex=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/LatexMapping.txt -prior_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/prior.p -true_params_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/true_params.p -plot_probes=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/plot_probes.p -true_expec_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/true_expec_vals.p -pmin=0 -pmax=8 -pmean=0.5 -psigma=0.3 -resource=0
=>> PBS: job killed: walltime 143423 exceeded limit 143395
