rm: cannot remove `dump.rdb': No such file or directory
Could not connect to Redis at 127.0.0.1:6383: Connection refused
+ job_id=7742778.master.cm.cluster
++ cut -d. -f1
+ job_number=7742778
+ echo 'Job id is 7742778'
+ echo 7742778
+ cd /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/logs
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/logs
+ REDIS_URL=redis://node35-013:6383
+ QMD_LOG_DIR=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs
+ OUTPUT_ERROR_DIR=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs
+ mkdir -p /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs
+ QMD_JOB=very_long_qhl_experimental_data_83
+ echo 'PBS job name is very_long_qhl_experimental_data_83'
+ QMD_LOG=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_83.qmd.7742778.log
+ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742778.master.cm.cluster
++ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742778.master.cm.cluster
+ export nodes=node35-013
+ nodes=node35-013
++ cat /cm/local/apps/torque/4.2.4.1/spool/aux//7742778.master.cm.cluster
++ wc -l
+ export nnodes=1
+ nnodes=1
+ export confile=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_83.conf
+ confile=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_83.conf
+ for i in '$nodes'
+ echo node35-013
+ cd /panfs/panasas01/phys/bf16951/QMD/Libraries/QML_lib
+ [[ node35-013 == \n\o\d\e* ]]
+ echo 'Launching RQ worker on remote nodes using mpirun'
+ printf 'In RUN script:  -agr non_interacting_ising'
+ mpirun -np 1 -machinefile /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/output_and_error_logs/node_info.very_long_qhl_experimental_data_83.conf rq worker 83 -u redis://node35-013:6383
+ ((  0 == 1 ))
+ sleep 5
+ cd /panfs/panasas01/phys/bf16951/QMD/ExperimentalSimulations
++ date +%H:%M:%S
+ echo 'Starting Exp.py at 15:29:26; results dir: /panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14'
+ echo 'CONFIG: -p=20000 -e=3000 -bt=1500 -rt=0.5 -ra=0.8 -cb=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/cumulative.csv -pt=1 -pgh=1.0 -qid=83 -rqt=10000 -pkl=0 -host=node35-013 -port=6383 -dir=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14 -log=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_83.qmd.7742778.log cpr=1 -ds=NVB_rescale_dataset.p -dst=5000 -dto=205 -ggr=two_qubit_ising_rotation_hyperfine_transverse'
+ python3 Exp.py -rq=1 -op=xTiPPyTiPPzTiPPxTxPPyTyPPzTz -p=20000 -e=3000 -bt=1500 -rt=0.5 -ra=0.8 -pgh=1.0 -qid=83 -rqt=200000 -g=1 -exp=1 -meas=hahn -qhl=1 -fq=0 -pt=1 -pkl=0 -host=node35-013 -port=6383 -dir=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14 -log=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/logs/very_long_qhl_experimental_data_83.qmd.7742778.log -cb=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/cumulative.csv -cpr=1 -ds=NVB_rescale_dataset.p -dst=5000 -dto=205 -ggr=two_qubit_ising_rotation_hyperfine_transverse -agr non_interacting_ising -latex=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/LatexMapping.txt -prior_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/prior.p -true_params_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/true_params.p -plot_probes=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/plot_probes.p -true_expec_path=/panfs/panasas01/phys/bf16951/QMD/ParallelDevelopment/Results/Nov_28/15_14/true_expec_vals.p -pmin=0 -pmax=8 -pmean=0.5 -psigma=0.3 -resource=0
=>> PBS: job killed: walltime 143413 exceeded limit 143395
